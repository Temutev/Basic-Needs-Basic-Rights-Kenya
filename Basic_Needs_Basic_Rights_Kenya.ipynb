{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Needs Basic Rights Kenya ",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUlpDao-B9V6"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaJYFcxVCMFW"
      },
      "source": [
        "train = pd.read_csv('Train.csv')\r\n",
        "test = pd.read_csv('Test.csv')\r\n",
        "sub = pd.read_csv('SampleSubmission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B-jW08bD0Ud"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHA8msepD1xB"
      },
      "source": [
        "print(train.shape)\r\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LipVWKcmESsQ"
      },
      "source": [
        "train.label.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xSVAhybEbIk"
      },
      "source": [
        "#Data Preprocessing \r\n",
        "# a mapping dictionary that maps the label values from 0 to 3\r\n",
        "label_mapping = {\r\n",
        "\"Depression\": 0,\r\n",
        "\"Drugs\": 1,\r\n",
        "\"Suicide\": 2,\r\n",
        "\"Alcohol\": 3\r\n",
        "}\r\n",
        "\r\n",
        "train[\"label\"] = train.label.map(label_mapping)\r\n",
        "\r\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhIXXI6TEwtW"
      },
      "source": [
        "\r\n",
        "train.label.value_counts().plot.barh()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuWbCaChE6G5"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WECCGnFsFcso"
      },
      "source": [
        "trainc= train.copy()\r\n",
        "testc=test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE49Fn-3Ffcw"
      },
      "source": [
        "import string\r\n",
        "import re\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\r\n",
        "    and remove words containing numbers.'''\r\n",
        "    text = text.lower()\r\n",
        "    text = re.sub('\\[.*?\\]', '', text)\r\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\r\n",
        "    text = re.sub('<.*?>+', '', text)\r\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\r\n",
        "    text = re.sub('\\n', '', text)\r\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\r\n",
        "    return text\r\n",
        "\r\n",
        "\r\n",
        "# Applying the cleaning function to both test and training datasets\r\n",
        "train['text'] = train['text'].apply(lambda x: clean_text(x))\r\n",
        "test['text'] = test['text'].apply(lambda x: clean_text(x))\r\n",
        "\r\n",
        "# Let's take a look at the updated text\r\n",
        "train['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI0xYhORFvhT"
      },
      "source": [
        "# Tokenizing the training and the test set\r\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\r\n",
        "train['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\r\n",
        "test['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\r\n",
        "train['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iprc6ZkHQ2q"
      },
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag8fXFiIGJbM"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "def remove_stopwords(text):\r\n",
        "    \"\"\"\r\n",
        "    Removing stopwords belonging to english language\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    words = [w for w in text if w not in stopwords.words('english')]\r\n",
        "    return words\r\n",
        "\r\n",
        "\r\n",
        "train['text'] = train['text'].apply(lambda x : remove_stopwords(x))\r\n",
        "test['text'] = test['text'].apply(lambda x : remove_stopwords(x))\r\n",
        "train.head()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIZ-YK_sHI5B"
      },
      "source": [
        "# After preprocessing, the text format\r\n",
        "def combine_text(list_of_text):\r\n",
        "    '''Takes a list of text and combines them into one large chunk of text.'''\r\n",
        "    combined_text = ' '.join(list_of_text)\r\n",
        "    return combined_text\r\n",
        "train['text']=train['text'].apply(lambda x: combine_text(x))\r\n",
        "test['text'] = test['text'].apply(lambda x : combine_text(x))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Medj17TKIKFR"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Y_Ooq3ID_o"
      },
      "source": [
        "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\r\n",
        "train_vectors = tfidf.fit_transform(train['text'])\r\n",
        "test_vectors = tfidf.transform(test[\"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEupuqaLITie"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from sklearn.metrics import log_loss\r\n",
        "\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\r\n",
        "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE7CAsK1IUDY"
      },
      "source": [
        "#split our data into train and test\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "#split features and target from train data \r\n",
        "X = train_vectors\r\n",
        "y = train.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etYdGpf4IbZW"
      },
      "source": [
        "\r\n",
        "# split data into train and validate\r\n",
        "\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\r\n",
        "    X,\r\n",
        "    y,\r\n",
        "    test_size=0.20,\r\n",
        "    random_state=42,\r\n",
        "    shuffle=True,\r\n",
        "    stratify=y,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7mDdS1NIey7"
      },
      "source": [
        "\r\n",
        "# Create a classifier\r\n",
        "news_classifier = MultinomialNB()\r\n",
        "\r\n",
        "# train the news_classifier \r\n",
        "news_classifier.fit(X_train,y_train)\r\n",
        "# test model performance on valid data \r\n",
        "y_probas = news_classifier.predict_proba(X_valid)\r\n",
        "# evalute model performance by using log_loss in the validation data\r\n",
        "log_loss(y_valid, y_probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K106xeCTIhnJ"
      },
      "source": [
        "\r\n",
        "rfr=RandomForestClassifier(n_estimators=150, max_depth=6, random_state=0)\r\n",
        "rfr.fit(X_train,y_train)\r\n",
        "rfr_probas= rfr.predict_proba(X_valid)\r\n",
        "log_loss(y_valid,rfr_probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP1XFN74IuZO"
      },
      "source": [
        "\r\n",
        "xgb=XGBClassifier()\r\n",
        "xgb.fit(X_train,y_train)\r\n",
        "xgb_probas= xgb.predict_proba(X_valid)\r\n",
        "log_loss(y_valid,xgb_probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA5JIS_RIwia"
      },
      "source": [
        "#logistic regression\r\n",
        "logr=LogisticRegression(max_iter=150,C=7, random_state=23)\r\n",
        "logr.fit(X_train,y_train)\r\n",
        "logr_probas= logr.predict_proba(X_valid)\r\n",
        "log_loss(y_valid,logr_probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peIOnMNNIzKM"
      },
      "source": [
        "\r\n",
        "# create prediction from the test data\r\n",
        "xgb_probass =xgb.predict_proba(test_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-69QqioMaQt"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gz5GC36Mad-"
      },
      "source": [
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGzhzxkWJwLC"
      },
      "source": [
        "# create submission file \r\n",
        "submission_cols = ['Depression', 'Alcohol', 'Suicide', 'Drugs'] \r\n",
        "submission_df = pd.DataFrame(xgb_probass, columns = submission_cols)\r\n",
        "submission_df['ID'] = test['ID']   # add  test_id \r\n",
        "\r\n",
        "#rearange columns \r\n",
        "submission_df = submission_df[['ID','Depression', 'Alcohol', 'Suicide', 'Drugs']]\r\n",
        "\r\n",
        "# save submission file \r\n",
        "submission_df.to_csv(\"first_submission.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-Prsr4ONBNh"
      },
      "source": [
        "submission_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNgpgBffNDvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}